# ğŸ§  Medical RAG Chatbot

> â€œInformation is not knowledge.â€  
> â€” *Albert Einstein*

---

## ğŸ“˜ Overview

**Medical RAG Chatbot** is a command-line assistant designed to answer evidence-based medical questions using scientific papers (e.g., PubMed). It leverages **Retrieval-Augmented Generation (RAG)** and integrates vector databases, LLMs (like GPT or Ollama), and user feedback for enhanced precision and trustworthiness.

All responses are grounded strictly in retrieved scientific content â€” ideal for clinicians, researchers, or medical students.

---

## ğŸ—‚ï¸ Project Structure

```
medical-rag-chatbot/
â”‚
â”œâ”€â”€ app/                            # ğŸ“¦ Main application module
â”‚   â”œâ”€â”€ cli/                        # ğŸ–¥ï¸ CLI entrypoint
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ cli.py
â”‚   â”œâ”€â”€ configs/                    # âš™ï¸ Paths, prompts, and key management
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ load_api_key.py
â”‚   â”‚   â”œâ”€â”€ paths.py
â”‚   â”‚   â””â”€â”€ prompts.py
â”‚   â”œâ”€â”€ core/                       # ğŸ” Core logic: RAG + vectorization
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rag.py
â”‚   â”‚   â””â”€â”€ vectorization.py
â”‚   â”œâ”€â”€ models/                     # ğŸ¤– LLM and embedding model wrappers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chatting_model.py
â”‚   â”‚   â””â”€â”€ embedding_model.py
â”‚   â””â”€â”€ services/                   # ğŸ”§ Feedback, auto-update, and resets
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ auto_update.py
â”‚       â”œâ”€â”€ feedback.py
â”‚       â””â”€â”€ reset.py
â”‚
â”œâ”€â”€ data/                           # ğŸ“„ Folder to store PDF documents
â”œâ”€â”€ docs/                           # ğŸ“š Optional documentation
â”œâ”€â”€ tests/                          # âœ… Unit and integration tests
â”œâ”€â”€ .env.template                   # ğŸ” Template for environment variables
â”œâ”€â”€ .gitignore                      # ğŸš« Git exclusions
â”œâ”€â”€ main.py                         # ğŸš€ App entrypoint (runs the CLI)
â”œâ”€â”€ README.md                       # ğŸ“˜ Project documentation
â””â”€â”€ requirements.txt                # ğŸ“¦ Python dependencies
```

---

## ğŸš€ Features

- ğŸ” Vector-based semantic search over local PDF documents
- ğŸ§  LLM-powered answers with grounded scientific context
- ğŸ’¬ Feedback memory system to refine response behavior
- ğŸ“ Auto-indexing of newly added or updated PDFs
- ğŸ§ª Supports both OpenAI and local embedding/LLM engines
- ğŸ§¹ Flexible database reset via CLI flags

---

## ğŸ“¦ Installation

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

Optional (for markdown or unstructured content):

```bash
pip install "unstructured[md]"
```

---

### 2. Environment Setup

Copy the environment template and add your API keys:

```bash
cp .env.template .env
```

Edit `.env`:

```env
OPENAI_API_KEY=your-key-here
```

---

## ğŸ’¡ Usage

### Ask a question:

```bash
python3 main.py ask "What is the role of dopamine in Parkinson's?"
```

### Add user feedback:

```bash
python3 main.py feedback "Explain each finding more clearly and use numbered lists."
```

### Reset databases:

```bash
python3 main.py reset --all         # Full reset
python3 main.py reset --em          # Reset embedding DB
python3 main.py reset --fb          # Reset feedback DB
python3 main.py reset --fi          # Reset file index
```

---

## ğŸ§  Feedback System

- Feedback is stored in `feedback/feedback.json`.
- Every RAG prompt includes all historical feedback.
- Reinforces helpful patterns and filters unwanted ones over time.

---

## ğŸ§¾ Prompt Template

The system uses a carefully formatted instruction template to ensure precise, grounded outputs:

```
You are a professional AI assistant answering scientific questions based strictly on provided content. 
You are dealing with medical questions where precision matters the most. 
All answers must be strictly based on the given scientific excerpts. 
You must not generate opinions or make assumptions. 
If the context lacks sufficient detail, respond with "I don't know" or a request for more information.

---

Context:
{context}

---

Feedback:
{feedback}

---

Question:
{question}
```

---

## ğŸ“Œ Chunking Strategy

- Uses `RecursiveCharacterTextSplitter`
- Parameters:
  - `chunk_size = 1000`
  - `chunk_overlap = 150`
- Each chunk is assigned a unique ID: `source:page:index`

---

## ğŸ§  Future Enhancements

- ğŸ” Hybrid retrieval (BM25 + vector search)
- ğŸŒ Web interface
- ğŸ“„ Summarization of multi-document answers
- âš—ï¸ Domain-specific model fine-tuning
- ğŸ§¬ Multi-modal inputs (e.g., images, figures)

---

## ğŸ” License

This project is open to everyone!
Feel free to use, edit, copy, paste, share, or even taste it â€” seriously, go wild! ğŸ½ï¸

Still, Always Be Civil (ABC)!

---

## ğŸ™ Acknowledgements

- Prof. Mark Turner at Case Western Reserve University
- Case Western Reserve University HPC
- Pixegami (https://github.com/pixegami)
- PubMed
- OpenAI
- Ollama
- LangChain
- ChromaDB